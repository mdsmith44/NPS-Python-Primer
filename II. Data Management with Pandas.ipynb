{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>NPS Python for Data Analysis Primer</center>\n",
    "## <center> <img src='Images/NPS_Logo.jpg' height=250/></center>\n",
    "<center style=\"font-size:24px\">LTC Matt Smith</center>\n",
    "<center style=\"font-size:24px\">NPS Operations Research Dept.</center>\n",
    "<center>matthew.smith@nps.edu</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2: Data Processing with Numpy and Pandas\n",
    "\n",
    "### A Brief History of Python (for data analysis...)\n",
    "\n",
    "<img src='Images/python_numpy_pandas.jpg' height=200/>\n",
    "\n",
    "- **1991:** Python first emerged as general purpose, high-level language\n",
    "- **2006:** NumPy (Numerical Python) provides optimized tools to store and operate on numerical data\n",
    "- **2010** Pandas, built on NumPy, emerges as essential tool for managing, processing, and analyzing data\n",
    "- **2010-present:** Python has grown as an increasingly popular language for data analysis, with a robust and active development community and userbase in many areas including:\n",
    "    - Machine Learning (scikit-learn) \n",
    "    - Natural Language Processing (nltk, Spacy, gensim)\n",
    "    - Deep Learning (PyTorch, Tensorflow, transformers)\n",
    "    - Data Visualization (matplotlib, seaborn, bokeh, plotly, streamlit)\n",
    "    - And much more...\n",
    "    \n",
    "    \n",
    "> ### NumPy serves as the building block for a large ecosystem of python packages for data science and other scientific applications, and in many ways helped usher in the growth of python as a tool for data analysis.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In This Notebook:\n",
    "\n",
    "- [Numpy](#Numpy)\n",
    "    - Overview of Numpy for numerical computing, including array creation, manipulation, and mathematical operations.\n",
    "- [Pandas Data Structures: DataFrame and Series](#Pandas-Data-Structures:-DataFrame-and-Series)\n",
    "    - [Pandas Overview](#Pandas-Overview)\n",
    "    - [DataFrame and Series Objects](#DataFrame-and-Series-Objects)\n",
    "    - [Working with Data by Index and Columns](#Working-with-Data-by-Index-and-Column)\n",
    "    - [Removing Data](#Removing-Data)\n",
    "    - [Multi-Level Indexing](#Multi-Level-Indexing)\n",
    "    - [Common Series Methods](#Common-Series-Methods)\n",
    "    - [Filtering and Sorting Data](#Filtering-and-Sorting-Data)\n",
    "- [Getting Data](#Getting-Data)\n",
    "    - [From Files](#From-Files)\n",
    "    - [From User Generated Data](#From-User-Generated-Data)\n",
    "    - [Webscraping](#Webscraping)\n",
    "    - [Writing and Saving Data](#Writing-and-Saving-Data)\n",
    "- [Cleaning Data](#Cleaning-Data)\n",
    "    - [Handling Nulls](#Handling-Nulls)\n",
    "    - [Using Where Functions](#Using-Where-Functions)\n",
    "- [Data Wrangling and Manipulation](#Data-Wrangling-and-Manipulation)\n",
    "    - [Applying Functions and Transformations](#Applying-Functions-and-Transformations)\n",
    "    - [Joining/Merging Data Sets](#Joining/Merging-Data-Sets)\n",
    "    - [Aggregating Data: Groupby and Pivot Tables](#Aggregating-Data:-Groupby-and-Pivot-Tables)\n",
    "- [Handling Dates and Times](#Handling-Dates-and-Times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy\n",
    "Numpy, short for numerical python, is a popular package for numerical computing in python.  It provides data structures, algorithms, and computation for working with arrays and other scientific applications.  It is also the foundation on which pandas, scikit-learn, and many other python packages are built and in many ways helped usher in the growth of python as a tool for data analysis. \n",
    "\n",
    "We show a few examples of how to use numpy below.  For more information, see the [Numpy documentation](https://numpy.org/doc/stable/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main data type is ndarray\n",
    "import numpy as np\n",
    "\n",
    "a = np.array([1,2,3,4]) #1d array\n",
    "b = np.array([[1,2],[3,4]]) #2d array\n",
    "print(\"a = \",a)\n",
    "print(\"b = \",b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you recall from earlier, python's built-in range method only takes interger inputs.  Numpy has a method `np.arange()` that improves that functionality, allowing any numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.arange useful way to get a sequence (range) of values\n",
    "#Specify np.arange(start,end,step), where sequence stops BEFORE reaching end value\n",
    "print('np.arange(5) = ',np.arange(5))\n",
    "print('np.arange(2,5) = ',np.arange(2,5))\n",
    "print('np.arange(2.5,5,.5) = ',np.arange(2,5,.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.linspace gives us sequence with desired number of equally spaced values\n",
    "print('np.linspace(1,5,15) = ',np.linspace(1,5,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Also useful is zeros or ones\n",
    "a = np.zeros((2,3))\n",
    "print(a)\n",
    "b = np.ones((3,2))\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np can do matrix multiplication with the @ symbol\n",
    "b@a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random has a lot of useful tools to generate random numbers\n",
    "\n",
    "#Create 10 random uniform samples from 0 to 1\n",
    "np.random.random(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create 20 random int's from 1 to 10 \n",
    "a = np.random.randint(low=1,high=11,size=20)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Also many built in math operators\n",
    "np.mean(a), np.std(a), np.median(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numpy will also broadcast a function to each element of the array if it makes sense to do so\n",
    "np.power(a,2) #Square each element of the array a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`np.hstack` and `np.vstack` are useful ways to flatten out a list of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_list = [[1,2,3],4,(5,6)]\n",
    "num_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get this as one flat list:\n",
    "np.hstack(num_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas Data Structures: DataFrame and Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pandas Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pandas library provides a wealth of tools and data structures to facilitate reading, cleaning, manipulating, processing, and analyzing data, and is largely responsible for the popularity of python as a data analysis platform.  Since becoming an open source project in 2010, pandas has matured and now integrates well with many other python packages such as matplotlib for data visualization, scikit-learn for machine learning, statsmodels for statistical analysis, and many others.  \n",
    "\n",
    "<img src='Images/angry_panda.gif'/>\n",
    "\n",
    "This notebook covers the main functionality of the pandas library.  It is certainly not an exhaustive treatment of all the features of the library, but highlights some of the commonly used aspects.  See the [pandas documentation](https://pandas.pydata.org/pandas-docs/stable/) for a full reference.  I also hihgly recommend the this book written by the creator of pandas, Wes McKinney's [Python for Data Analysis](https://learning.oreilly.com/library/view/python-for-data/9781491957653/ch11.html) book on O'Reily.  \n",
    "\n",
    "Pandas is built on the numpy library and incorporates much of numpy's functionality.  However, it's still useful to import both libraries since numpy will provide some extra functionality to help run computations and process numerical data.  Hence, every data analysis notebook I ever use typically starts with the following two imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame and Series Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two main pandas data structures are DataFrame (for tabular data) and Series (single column/vector of data).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in and view the first 5 rows of a DataFrame\n",
    "#This is some fake Physical Training (PT) Test scores including scores (0-100) for Pushups, Situps, and 2Mile run\n",
    "df = pd.read_csv('Data/ptScores.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each column of a DataFrame is a pandas `Series` object.  We can access a single column with `df[\"col_name\"]`, or if the column name has no spaces we can also access it with `df.col_name`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Each column of the DataFrame is a Series\n",
    "ser = df.age\n",
    "ser.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(df),type(ser))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the DataFrame is essentially a tabular, spreadsheet-like data structure.  Unlike numpy arrays which must be a single data type, DataFrames can hold multiple different data types.  However, each `Series` (that is, each column of the `DataFrame`) does have a single data type.\n",
    "\n",
    "We can see the data types of each column with the info() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View basic info of DataFrame, including data type of each column\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are two numerical types for float and int data.  There are also other pandas data types for boolean or datetime data.  Everything else, including string data, is classified as \"object\" data type.\n",
    "\n",
    "It's also helpful to get a quick summary of any numerical columns with the describe() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get quick summary of numerical fields\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Data by Index and Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have noticed that DataFrames labels all rows and columns.  Row labels are the DataFrame index while column labels are the DataFrame columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding how to effectively use the Index and Columns is essential to being able to view, explore, interact with, and manipulate your data. Here are a few commonly used techniques for using index and column that may be helpful.\n",
    "\n",
    "Quick note on terminology.  While *index* refers to the row labels and *columns* refers to the column labels, both index and column are pandas *Index* objects.  Hence, sometimes the term \"index\" refers specifically to the row index values, and sometimes the term \"index\" can refer generally to either index or column.  For example, the function `df.reindex()` can re-assign the labels for either the rows or column depending on the arguments you pass in.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly view various subsets of a DataFrame with head(), tail() or sample() methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View first n rows\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View last n rows\n",
    "df.tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View random sample\n",
    "df.sample(n=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify a set or range of rows to display by referencing the index of each row, either using `df.loc[]` to locate rows by index name, or using `df.iloc[]` to locate rows by index numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View specific range of rows\n",
    "df.iloc[5:10] #View rows number 5, 6, 7, 8, and 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leave off start index to default to start of df\n",
    "df.iloc[:6] #rows 0 to 5, same as df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leave off end index to go through end of df\n",
    "df.iloc[len(df)-4:] #Same as df.tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also specify a single row, in which case iloc returns a Series object with the data of that row\n",
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can feed in any collection of row numbers\n",
    "df.iloc[[0,10,50,100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unfortunately we can't feed in a combination of multiple different ranges\n",
    "#df.iloc[[0:4,10:15]] #This won't work\n",
    "\n",
    "#One trick to view multiple ranges is with the np.r_ function to get a custom range\n",
    "#E.g., here's a trick I use often to view first few and last few rows of a DataFrame\n",
    "df.iloc[np.r_[0:4,-4:0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned, we can also view rows by index name with df.loc[].  However, currently the *name* of each df row is the same as the *number* of each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[0:3] #Should give similar result as df.iloc[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the distinction between df.iloc and df.loc clearer, lets rename the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can get the current df index with df.index, which returns a pandas RangeIndex object\n",
    "df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This may seem scary, but we can always convert any index range to a list\n",
    "list(df.index)[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can set index to new values by providing a sequence the same length as df\n",
    "df.index = np.arange(200,200+len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Or we can set index to be a current column of the df\n",
    "df.set_index('SoldierInitials')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm, the index didn't actually get changed.  What happened?  Turns out most pandas funtions only return a new value of the dataframe for the user to use or discard; they don't actually change the dataframe object in place.  Hence, it's usually needed to re-assign the updated dataframe to itself to actually update it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('SoldierInitials')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That did the trick.  Two points worth noting about the index:\n",
    "1. Index doesn't have to be an integer.  Here it's a string, and we'll see later it can also be a datetime.  \n",
    "2. The index can have a name.  This can be a useful way to keep track of what the index name refers to.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can locate df rows by index name with df.loc[]\n",
    "df.loc['LB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[['LB','EV']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another point about Index is that the values don't have to be unique.  We can use the value_counts() pandas method to see that several of the index values are repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the 5 most repeated values in the index\n",
    "df.index.value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Locating any repeated index will return all matching rows\n",
    "df.loc[['NU','TS']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view a subset of the DataFrame columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns #Returns Index object, a sequence of column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View subset of columns\n",
    "df[['PU','SU','MI2']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If we grab a single column, it will return a Series\n",
    "df['Average'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can grab a single column but remain a DataFrame by feeding in a list of a single element\n",
    "df[['Average']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use df.iloc[] or df.loc[] to re-assign values in a dataframe row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Must feed in a sequence that matches the number of columns\n",
    "df.iloc[0] = ['M','O5',50,100,100,100,100]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Works for loc as well\n",
    "df.loc['LB'] = ['M','O5',50,90,90,90,90]\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can **add** new rows with the loc[] method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also feed in an index name that doesn't exist to df.loc[] and it will creat a new row\n",
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc['ZZ'] = ['M','O5',50,90,90,90,90]\n",
    "df.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you only specify a single value when adding a new row, pandas will \"Broadcast\" that value, i.e. repeat it enough times to fill the entire row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you only specify a single value, pandas will \"Braodcast\" that value\n",
    "df.loc['ZZZ'] = '5'\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Unfortunately we can't use iloc or loc to change a single entry in the df\n",
    "df.loc['ZZ']['age'] = 100 #This wont' update the age of the ZZ entry\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pandas provides an at functionality for this purpose\n",
    "#Feed in index name followed by column name\n",
    "df.at['ZZ','age'] = 100\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add new Columns in the same way, either by assigning it to a vector of the correct length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Pass'] = ['Yes']*len(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Or we can again specify one value, and pandas will broadcast it across the entire column\n",
    "df['Pass'] = 'No'\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also change the column or index names with rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'MI2':'2MileRun','PU':'Pushups'}).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Removing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We've seen how to *keep* or even rearrange a subset of columns or indices, e.g. with `df[cols_to_keep]` or `df.loc[rows_to_keep]`.  We can also specify what index values to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Recall current value of df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Remove given entries with df.drop()\n",
    "#Default is to remove entries from Index (axis=0)\n",
    "df_new = df.drop(['LB','SW'])\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Set axis argument to 1 to remove values from columns\n",
    "df_new = df.drop(['gender','rank'],axis=1)\n",
    "df_new.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Instead of the confusing \"axis=1\" notation, we can directly specify the list of items to remove from the index or column, which can be more intuitive.  It's always great to make your code more intuitive, especially if someone else will look at your code later, such as another data analyst, or even future you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.drop(index=['LB','SW']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.drop(columns=['gender','rank']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#We can even use this notation to drop both index and column labels\n",
    "df.drop(index=['LB','SW'],columns=['gender','rank']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "One tricky part of the drop() method is that if we feed in a value that is not in the index, it will throw an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#This will throw an error, since 'Pass' is not in the columns\n",
    "#df.drop(['gender','rank','Pass'],axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Another tool we can use, which helps get around this issue, is to use the difference() method of pandas Index objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#This will only keep columns that are not in the list ['gender','rank','Pass']\n",
    "cols_to_keep = df.columns.difference(['gender','rank','Pass'])\n",
    "#This will run just fine even though 'Pass' is not in the columns\n",
    "df[cols_to_keep].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Yet another method we can use to remove and narrow down data (there's always about 10 ways to do any task in python/pandas) is with the reindex() method, where we can specify the new set of indices or columns to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Use reindex() to specify which index or column values to keep, and the order\n",
    "df.reindex(columns=['age','MI2','SU','PU']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "One advantage of reindex is that we can pass in values that are currently not in the index, and it will initialize a new row/column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#HairColor is not in the current columns, so this will initialize a new column, \n",
    "# with all null values for now\n",
    "df.reindex(columns=['age','MI2','SU','PU','HairColor']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Multi-Level Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Pandas supports mutli-level indexing.  This is a more advanced topic, so feel free to skim this section.  However, multi-level indexing is a powerful tool that allows you to work with more complex data structures or group by multiple data features at once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/ptScores.csv')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#set 3-level index\n",
    "df_multi = df.set_index(['SoldierInitials','gender','rank'])\n",
    "df_multi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each index is 3-tuple\n",
    "df_multi.head().index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can access rows by specifying the index values in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_multi.loc['ML']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_multi.loc['ML','M','O5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#We can use reset_index() to \"unpack\" indices back into columns\n",
    "df_multi.reset_index().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#We can also drop (wipe out) the index values with reset_index(drop=True)\n",
    "df_multi.reset_index(drop=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Series Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas has many built-in Series methods that are useful for summarizing or evaluating the data in that column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recall df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute variety of statistics of any numerical column\n",
    "df.age.min(), df.age.max(), df.age.mean(), df.age.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform arithmatic on any series\n",
    "(10*df.age+3).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Boolean Series wherever value satisfies some condition\n",
    "(df.age<30).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any string columns, we can access any standard python string methods using `df.col_name.str.method()`.  This will typically return a new series where the string method is applied element-wise to each entry in the original series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get new Series with length of strings in SoldierInitials column\n",
    "df.SoldierInitials.str.len().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get new Boolean Series which is True for any entries that contain an \"L\"\n",
    "df.SoldierInitials.str.contains('L').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some more useful Series methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get array of unique values in any column\n",
    "df['rank'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: Why did we run `df['rank'].unique()` instead of `df.rank.unique()`?  Turns out that `rank` is a DataFrame method, so when we type `df.rank`, pandas first tries to execute the rank() method (which retruns a new dataframe where each element is replaced by its ranking within that column, try it out below to see).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get list of value counts in any Series\n",
    "df['rank'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace values by some mapping\n",
    "gender_map = {'M':'Male','F':'Female'}\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Filtering and Sorting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We just saw that we can evaluate Boolean Series indicating where the values of a Series satisfy some condition.  We can use this to quickly filter our DataFrames to only show data satisfying some condition, a simple but powerful technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Recall df\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Only view Soldiers under 20\n",
    "df[df.age<20].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Only view Officers\n",
    "df[df['rank'].str.startswith('O')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To *negate* a boolean Series, we have to use the ~ symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df[~ df['rank'].str.startswith('O')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#We can combine multiple conditions\n",
    "#E.g. see who failed at least one event (scored less than 60 points)\n",
    "df[(df.PU<60) | (df.SU < 60) | (df.MI2 < 60)].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As a warning on using multiple conditions, pandas may not be able to parse multiple boolean conditions if they are not contained in parentheses.  E.g., `df[df.PU<60 | df.SU < 60 | df.MI2 < 60]` will throw an error, so always good practice to include parentheses around each condition such as `df[(df.PU<60) | (df.SU < 60) | (df.MI2 < 60)]`.\n",
    "\n",
    "<blockquote>When in doubt, add parentheses throughout!</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can also sort our dataframe, a great way to quickly view or rearrange the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Sort alphabetical by initials\n",
    "df.sort_values(by='SoldierInitials').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Sort from oldest to youngest\n",
    "df.sort_values(by='age',ascending=False).head() #ascending=True is the default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Do multiple sorts\n",
    "df.sort_values(by=['rank','age'],ascending=[False,True]).head(10)\n",
    "#This will do a primary sort on rank in descedning order, and a secondary sort on age in ascending order\n",
    "#Hence, we start off with some high speed 20 yr old O6's....recall this is fake data :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It's worth noting that none of the above commands actually *changed* the `df` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To change the dataframe itself, we can either apply the update back to the object, `df = df.sort_values(..)`, or we can use the inplace argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df.sort_values(by=['rank','age'],ascending=[False,True],inplace=True)\n",
    "#df now sorted by rank in descending order, all the big wigs up top\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### From Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As we've seen, we can grab data from a csv file with the read_csv method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/ptScores.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Pandas also has built in methods to read in data from a variety of other data formats, including excel, json, and many others.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Uncomment to see the available read in methods\n",
    "#pd.read_*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### From User Generated Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Sometimes we want to initialize a DataFrame with some existing data.  For example, maybe we ran a set of simulations, and want to load these into a DataFrame so that we can use pandas tools to wrangle and evaluate the data we generated.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Generate or simulate some data\n",
    "grades = ['A','B','C','D','F']\n",
    "student_IDs = np.random.randint(1,100,size=10) #10 random student ID #'s\n",
    "student_grades = np.random.choice(grades,size=10) #10 random student grades\n",
    "\n",
    "#Feed in data as a dict mapping keys to data sequences\n",
    "#Keys become column names\n",
    "df_grades = pd.DataFrame(data={'Student_ID':student_IDs,'Grade':student_grades})\n",
    "df_grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#We can also feed in a 2D array\n",
    "x = np.random.randn(8,3)\n",
    "df_data = pd.DataFrame(data=x)\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#If you don't like column labels of 0, 1, and 2, can feed in column labels\n",
    "df_data = pd.DataFrame(data=x,columns=['Sim1','Sim2','Sim3'])\n",
    "df_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Webscraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Python has powerful tools to search and extract data from the web.  Consider the following url, which contains the results from the 2019 Masters golf tournamnet: https://www.espn.com/golf/leaderboard?tournamentId=401056527.  How could we extract the data from that website and get it into a DataFrame to capture the results and/or do further analysis?\n",
    "\n",
    "We can use the urllib library to read in raw data from url's.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Now use the request module to request the raw html code from our url\n",
    "url = 'https://www.espn.com/golf/leaderboard?tournamentId=401056527'\n",
    "\n",
    "#Open the url, and read in the raw html code\n",
    "html = str(urllib.request.urlopen(url).read())\n",
    "\n",
    "print(html[:500]) #Print first 500 characters of html code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If, like me, you are a human and not a computer, that probably looks like a hot mess.  If we wanted, we could try to parse through the html code to try and extract the data, perhaps with some sophisticated regex pattern searching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "start_of_table = html.find('<table')\n",
    "html[start_of_table:start_of_table+1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Thankfully, python has a package BeautifulSoup which helps parse through the mess that is html code.  Here's how we could use it to extract the table data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as BS\n",
    "soup = BS(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Find the table tag\n",
    "tables = soup('table')\n",
    "table = tables[0]\n",
    "#table is now a beautiful soup Tag object\n",
    "print(type(table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We could now try and parse through and grab the contents.  For example, to get the column names, we could sift through all `<th>` tags (table header) and grab their contents (text attribute) as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Grab column names from all <th> (table header) tags\n",
    "column_tags = table('th') #list of all <th> tags within the table tag\n",
    "col_labels = []\n",
    "for col in column_tags:\n",
    "    col_labels.append(col.text)\n",
    "df_masters = pd.DataFrame(columns=col_labels)\n",
    "df_masters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#And then we could go through all rows (<tr> tags) and extract the data for each table entry in that row (<td> tags)\n",
    "for tr in table('tr'):\n",
    "    curr_row_data = []\n",
    "    for td in tr('td'):\n",
    "        curr_row_data.append(td.text)\n",
    "    #Now add current row data to dataframe\n",
    "    if (len(curr_row_data) == len(col_labels)):\n",
    "        df_masters.loc[len(df_masters)] = curr_row_data\n",
    "\n",
    "#Lets see if that worked\n",
    "df_masters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Thankfully, for this particular task of grabbing data from html tables, pandas has built in function\n",
    "df_masters = pd.read_html(url)[0]\n",
    "#pd.read_html returns list of all tables.  Use [0] to grab the first table.\n",
    "df_masters.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Of course, things may not always be that simple.  Often times the content in web pages is not simply embedded in the html code but is generated through some dynamic javascript calls, in which case this approach won't work.  If you do encounter this problem, the `selenium` package can be very helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Can't scrape table data from this page\n",
    "url2 = 'https://www.masters.com/en_US/scores/index.html'\n",
    "# df_masters2 = pd.read_html(url2)[0]\n",
    "# df_masters2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "This only scratches the surface of what you can do with webscraping in python.  Often times a website will have an API to facilitate extraction of the key data, which python can handle well.  Python also has a good parser for XML content called [ElementTree](https://docs.python.org/2/library/xml.etree.elementtree.html).  Also can see the [urllib](https://pythonspot.com/urllib-tutorial-python-3/) and [BeautifulSoup](https://pypi.org/project/beautifulsoup4/) documentation for more information on those packages.  I would also recommend the Coursera Course [Using Python to Access Web Data](https://www.coursera.org/learn/python-network-data/home/welcome) which covers all these topics and more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Writing and Saving Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can also use pandas to write and save a dataframe to a new file.  This can be useful way to save updated results of any data analysis or manipulations.  For example we could save off the masters data we just read in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_masters.to_csv('2019_Masters.csv',index=False) \n",
    "#Set index=False to avoid getting extra column for index in csv file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often reported that data analysts will spend up to 80% of their time just getting, cleaning, and wrangling data, with only 20% left over for analyzing and gaining insights from data.  With data often pulled in from many different data sources, in many different formats and lots of missing values, data cleaning is a crucial skill for data analysis.  Pandas provides many fast and flexible tools to clean data and get it into the right form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas handles null values with the NaN notation (for Not a Number).  Lets read in some sample CDC health data, in which some entries are missing and come through as NaN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/cdcSmall.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use isnull() on a Series to see any entries where that row is null\n",
    "df.height.isnull().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Look at any entries of entire DataFrame where height is null\n",
    "df[df.height.isnull()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can also use isnull() on entire dataframe to find Nulls in any column\n",
    "df.isnull().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can use df.isnull() along with any() or all() methods to find rows with any (at least one) null or all nulls\n",
    "#View any rows with AT LEAST ONE null\n",
    "df[df.isnull().any(axis=1)].head() #Axis=1 to go across columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To return all entries where height is NOT null, \n",
    "# we could either negate the boolean Series with the ~ symbol\n",
    "df[~ df.height.isnull()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Or we could use the notnull() method on a Series or DataFrame\n",
    "df[df.height.notnull()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can fill in any null values with the fillna() method\n",
    "df.fillna('UNKONWN').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We could also apply fillna to each column\n",
    "#Here's how we could replace each Null by the most common value within that column\n",
    "for c in df.columns:\n",
    "    #Find most common entry in this column\n",
    "    most_common = df[c].value_counts().index[0]\n",
    "    print('Most common value for column {} is {}'.format(c,most_common))\n",
    "    df[c] = df[c].fillna(most_common)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And resulting df\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the value you fill in will be highly dependent on the problem at hand.  For example, you don't want to inject representative but fake data and have it be interpreted as real. \n",
    "\n",
    "We can also drop any entires with null data with `dropna()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-read original df with null values\n",
    "df = pd.read_csv('Data/cdcSmall.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop any rows with at least one null\n",
    "df.dropna().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We could also only drop rows where ALL values are null\n",
    "df.loc[0] = [np.nan]*len(df.columns)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will drop the first row (with all nulls), \n",
    "# but will not delete the rows with a single null\n",
    "df = df.dropna(how='all')\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Using Where Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Recall current dataframe\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The pandas where() function returns a new dataframe where some condition is met.  Where the condition is met, the original dataframe value are unchanged, and wherever condition is not met the original value is replaced by somethign else (default being NaN).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Return df where gender is 'm'\n",
    "df.where(df.gender=='m').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Specify fill/replacement values\n",
    "df.where(df.gender=='m',other='Not m').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As you can see, df.where() applies the same condition everywhere, and frankly I don't use it a whole lot.  However, I do frequently use the `np.where()` method.  One key reason is that np.where allows you to specify one value to return where condition is True, and another value to return if condition is False.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Consider PT Scores again\n",
    "df_pt = pd.read_csv('Data/ptScores.csv')\n",
    "df_pt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Say we wan't to add a column for Pass which is 'Yes' wherever PU, SU and MI are ALL >=60, and \"No\" otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#First, notice how we can get a boolean Series\n",
    "#Start by getting dataframe replacing all raw scores with booleans\n",
    "(df_pt[['PU','SU','MI2']]>=60).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Then convert to single Boolean Series with all() method\n",
    "(df_pt[['PU','SU','MI2']]>=60).all(axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Now feed this boolean series into np.where\n",
    "df_pt['Pass'] = np.where((df_pt[['PU','SU','MI2']]>=60).all(axis=1),\n",
    "                        'Yes',\n",
    "                        'No')\n",
    "df_pt.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Student Challenge**\n",
    "\n",
    "Find the 5 highest `Average` scores among those who did NOT pass the PT test (that is, `Pass == \"No\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling and Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Applying Functions and Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_pt = pd.read_csv('Data/ptScores.csv')\n",
    "df_pt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can apply functions to each row or column of a DataFrame with the apply() method.  For example, say we wanted a new column that contained a tuple ID of (SoldierInitials,gender,rank) for each Soldier.  We could use apply the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Define a function that will act on each row of the dataframe\n",
    "def add_ID(row):\n",
    "    #The function will treat each row as its own Series object, meaning we can access its values as follows\n",
    "    initials = row['SoldierInitials']\n",
    "    gender = row['gender']\n",
    "    rank = row['rank']\n",
    "    new_ID = (initials,gender,rank)\n",
    "    #Now create a new entry in the row\n",
    "    row['ID'] = new_ID\n",
    "    #And return updated row\n",
    "    return row\n",
    "\n",
    "#Now apply this function to df, using axis=columns to apply it to the columns\n",
    "df_pt = df_pt.apply(add_ID,axis='columns')\n",
    "df_pt.head()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It is also common to apply lambda functions.  We could accomplish the same task as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_pt['ID'] = df_pt.apply(lambda x: (x['SoldierInitials'],x['gender'],x['rank']),axis='columns')\n",
    "df_pt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can also apply a function to each individual element of a dataframe with applymap()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_scores = df_pt[['PU','SU','MI2']]\n",
    "df_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_scores = df_scores.applymap(lambda x: \"{:.2f}\".format(x)).head()\n",
    "df_scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There is also the map() method, which can be applied to a Series (single column) to use a function or a dict to map current value to new values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Convert each elemetn of MI2 back to int\n",
    "df_scores.MI2 = df_scores.MI2.map(lambda x: int(float(x)))\n",
    "df_scores.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Joining/Merging Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The `pd.merge()` method allows us to quickly combine two datasets based on some common info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "names1 = ['Alice','Bob','Charlie','Dwayne','Erin']\n",
    "df1 = pd.DataFrame(data={'name':names1,'age':np.random.randint(low=20,high=40,size=len(names1))})\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "names2 = ['Dwayne','Erin','Frank','Gina']\n",
    "df2 = pd.DataFrame(data={'name':names2,'height':np.random.randint(low=55,high=70,size=len(names2))})\n",
    "df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The two main steps to merging data:\n",
    "1. Specify what column or columns to merge on\n",
    "2. Specify what data to keep with the `how` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Merge on name column, and only keep entries found in BOTH datasets with how='inner'\n",
    "pd.merge(df1,df2,on='name',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Only keep data from the first/left dataframe with how='left'\n",
    "#For records not found in seoncd/right dataframe, it will return NaN\n",
    "pd.merge(df1,df2,on='name',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Can also keep just right entries\n",
    "pd.merge(df1,df2,on='name',how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Can also keep all records with how='outer'\n",
    "pd.merge(df1,df2,on='name',how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can also specify different field names if the same field is called 2 different things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df3 = df2.rename(columns={'name':'NAME'})\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.merge(df1,df3,left_on='name',right_on='NAME',how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Aggregating Data: Groupby and Pivot Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Grouping and pivoting data are some of the most useful methods for summarizing and extracting insights from data sets.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Read in the CDC data\n",
    "df_cdc = pd.read_csv('Data/cdcSmall.csv')\n",
    "df_cdc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#We'll groupby gender, so clean that column up\n",
    "df_cdc.gender.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_cdc.gender = df_cdc.gender.replace({'f':'F','m':'M'})\n",
    "df_cdc.gender.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Pandas follows the same Split-Apply-Combine methodology found in R and many other data analysis languages where we Split data by some field or set of fields, Apply a function to the data columns from each split, then Combine and aggregate the results.  \n",
    "\n",
    "The basic mechanics is `df.groupby([fields to split on])[fields to apply function to].function_to_apply()`.  Here's how it looks if we want to get average height by gender.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_cdc.groupby('gender')['height'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can also split by multiple fields, or aggregate on multiple fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Split by multiple fields\n",
    "df_cdc.groupby(['gender','genhlth'])['height'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Can also aggregate multiple columns\n",
    "df_cdc.groupby(['genhlth'])[['height','weight']].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Recall this is all made up / random data, so don't read anything into the numerical results here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We see that the returned object will next all the fields we split on as a multi-level index.  It is also common practice to get these fields back into the resulting dataframe with reset_index() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_cdc.groupby(['gender','genhlth'])[['height','weight']].mean().reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We can also apply custom lambda functions using the `agg()` method to feed in a custom function to aggregate on.  E.g. say we want the spread of each field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Aggregate by custom lambda function\n",
    "df_cdc.groupby(['gender'])[['height','weight']].agg(lambda x: x.max() - x.min()).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Note that the column names are now confusing since we are actually showing the range of height and weight.  It is also common to apply the pandas add_suffix() or add_prefix() method before resetting the index to add a clarifying label to each column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Re-label columns after groupby\n",
    "df_cdc.groupby(['gender'])[['height','weight']].agg(lambda x: x.max() - x.min()).add_suffix('_range').reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Pivot Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Similar to groupby, pivot tables allow us to aggregate data to get insightfully summarize the data.  The main difference is that pivot tables yield a 2-dimensional view, and can give an additional method for reshaping the data.\n",
    "\n",
    "The mechanics of pivot_tables are to specify:\n",
    "1. The index of the resulting dataframe ('index' argument)\n",
    "2. The columns of the resulting dataframe ('columns' argument)\n",
    "3. The fields to aggregate on ('values' argument)\n",
    "4. The function used to do the aggregation ('aggfunc' argument), where we can pass in a string such as \"max\" or \"mean\" for common built-in function, or even specify our own function.\n",
    "\n",
    "Here's how it looks to get pivot table showing average height by genhlth and gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_cdc.pivot_table(values='height', index='genhlth', columns='gender', aggfunc='mean')\n",
    "#Note that 'gender' is now the name of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#We can also speficy multiple fields to aggregate on\n",
    "#The result is a multi-level column\n",
    "df_cdc.pivot_table(values=['height','weight'], index='genhlth', columns='gender', aggfunc='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#We can even specify multiple aggregation functions\n",
    "df_cdc.pivot_table(values=['height','weight'], index='genhlth', columns='gender', aggfunc=['max','min'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Handling Dates and Times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Pandas has built in functionality to handle dates and times.  Before looking at the pandas functionality, it helps to first look at the python datetime module, which is the bases for how pandas deals with dates and times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#datetime object contains tuple of (Year, Month, Day, Hour, Minute, Microseconds)\n",
    "now = datetime.now()\n",
    "now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#We can access components of a datetime object\n",
    "now.year, now.minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#timedelta object gives difference between two times\n",
    "td = now - datetime(2020,1,1) #Time since start of the year\n",
    "td\n",
    "#timedelta ojbect is given as tuple of (days,seconds,microseconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "It is also common to build custom strings from datetime objects.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Convert datetime to string with strftime(), \"string from time\"\n",
    "now.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Convert string to datetime with strptime()\n",
    "#Note that we have to feed in the exact datetime format, which can be annoying.\n",
    "#Thankfully, as we'll see, pandas can automatially interpret a range of datetime strings\n",
    "datetime.strptime(\"2019-05-10\",'%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Pandas incorporates datetime and other functionality to be able to handle and filter by date and time data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Read in exampel weather data\n",
    "df_weather = pd.read_csv('Data/weather.csv')\n",
    "df_weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Notice that DATE column is currently a string (object dtype)\n",
    "df_weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#We can convert the column to a pandas datetime object with pd.to_datetime()\n",
    "df_weather.DATE = pd.to_datetime(df_weather.DATE)\n",
    "df_weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Now see the updated dtype\n",
    "df_weather.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#We can se the DATE as the index, in which case it becomes a DateTimeIndex object\n",
    "df_weather.set_index('DATE',inplace=True)\n",
    "df_weather.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_weather.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Grab just data for 2017\n",
    "df_DEC = df_weather.loc['2017']\n",
    "df_DEC.iloc[np.r_[0:4,-4:0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grab just data for DECEMBER of 2017\n",
    "df_DEC = df_weather.loc['2017-12']\n",
    "df_DEC.iloc[np.r_[0:4,-4:0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The pandas period object is also a useful way to specify a PERIOD of time rather than a specific point in time, which is what you get with a single timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.to_datetime(now) #Timestamp object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.Period(now,freq='M') #Monthly period, which will specify the entire time period of the current month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Add a pandas Timedelta object to any Timestamp or Period\n",
    "td = pd.Timedelta('7D') #7 Day Timedelta\n",
    "td"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "pd.to_datetime(now) + td #Timestamp for 7 days from now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Move Period ahead by given Timedelta\n",
    "pd.Period(now,freq='D') + pd.Timedelta('60D') #60 days ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#We can also create a custome date range with pd.date_range()\n",
    "#Ex: Get range of every other Sunday for 9 perios\n",
    "dates = pd.date_range('10-01-2016', periods=9, freq='2W-SUN')\n",
    "dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#We can also create a PeriodIndex\n",
    "pd.period_range('2000-01-01', '2000-06-30', freq='M')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "For much more on datetime functionality, see the python [datetime documentation](https://docs.python.org/3/library/datetime.html) and the [pandas datetime documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html).  I'd also recommend the datetime chapter of McKinney's [Python for Data Analysis](https://learning.oreilly.com/library/view/python-for-data/9781491957653/ch11.html) book on O'Reily.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
