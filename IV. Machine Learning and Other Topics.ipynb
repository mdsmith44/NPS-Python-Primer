{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In This Notebook:\n",
    "\n",
    "This notebook gives a quick overview and pointer to some of the common python data analysis libraries.  All of these and other packages are built on top of the libraries covered in the first three lessons (numpy, pandas, and matplotlib).  \n",
    "\n",
    "- [Machine Learning with scikit-learn](#Machine-Learning-with-scikit-learn)\n",
    "    - [Scikit-Learn Overview Using Linear Regression](#Scikit-Learn-Overview-Using-Linear-Regression)\n",
    "    - [Data Prep](#Data-Prep)\n",
    "        - [Data Reduction with PCA](#Data-Reduction-with-PCA)\n",
    "        - [Train-Test Splits](#Train-Test-Splits)\n",
    "    - [Classification](#Classification)\n",
    "        - [Multi-Layer Perceptron Neural Nets (MLPs)](#Multi-Layer-Perceptron-Neural-Nets)\n",
    "        - [Logistic Regression](#Logistic-Regression)\n",
    "        - [Random Forest](#Random-Forest)\n",
    "        - [Decision Trees](#Decision-Trees)\n",
    "        - [K Nearest Neighbors](#K-Nearest-Neighbors)\n",
    "    - [Cross Validation with GridSearch](#Cross-Validation-with-GridSearch)\n",
    "    - [Unsupervised Learning / Clustering](#Unsupervised-Learning-/-Clustering)\n",
    "- [Other Data Analysis Tools](#Other-Data-Analysis-Tools)\n",
    "    - [Natural Lanugage Processing with nltk](#Natural-Language-Processing-with-nltk)\n",
    "    - [Network Analysis with networkx](#Network-Analysis-with-networkx)\n",
    "    - [Statistical Modeling and Time Series Analysis with statsmodel](#Statistical-Modeling-and-Time-Series-Analysis-with-statsmodel)\n",
    "    - [Optimization](#Optimization)\n",
    "    - [Simulation](#Simulation)\n",
    "    - [Association Analysis](#Association-Analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run to load packages before jumping to other parts of the notebook\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since its inception in 2010, [Scikit-learn](https://scikit-learn.org/stable/index.html) has become the premier python toolkit for machine learning.  It includes built-in functions and models for all types of machine learning tasks, including regression, classification, clustering, dimension reduction, parameter tuning, model selection, feature extraction, and more.  \n",
    "\n",
    "We give an overview of the main mechanics and some examples in this notebook.  For additiona reference I'd recommend the Coursera [Applied Machine Learning in Python](https://www.coursera.org/search?query=applied%20machine%20learning%20python&) course, or [Introduction to Machine Learning with Python](https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/) by Andreas C. MÃ¼ller and Sarah Guidomon on O'Reily is also a great resource."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn Overview Using Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scki-kit makes it extremely easy to fit machine learning models.  Lets start with a simple linear regression.  Using the conventional notation, we assume we have $n$ data points, each with $p$ features $(x_1, x_2, ..., x_p)$ and an output value $y$.  In some cases $y$ only takes on discrete non-numerical values, in which case we have a **Classification** problem, or when $y$ is numerical we have a **Regression** problem.  Both classifiation and regression are types of **Supervised Learning** problems where we have known output values, but we can also have **Unsupervised Learning** where there are no outputs, only input values of the features from which we can learn patterns or identify clusters and anomalies in the data.  \n",
    "\n",
    "For now lets look at regression, and in particular linear regression (aka Oridinary Least Squares) where we assume a linear model mapping inputs to outputs.  That is, for data point $i$, we have\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... + \\beta_p x_{ip}$$\n",
    "\n",
    "In matrix equation form it then looks like this:\n",
    "\n",
    "$$\n",
    "y = \\begin{bmatrix}y_1 \\\\ y_2 \\\\ \\vdots \\\\ \\\\ y_n \\end{bmatrix}_{n \\times 1}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 & x_{1,1} & x_{1,2} & \\ldots & x_{1,p} \\\\\n",
    "1 & x_{2,1} & x_{2,2} & \\ldots & x_{2,p} \\\\\n",
    "\\vdots  & \\ddots \\\\\n",
    "\\vdots \\\\\n",
    "1 & x_{n,1} & x_{n,2} & \\cdots & x_{n,p}\n",
    "\\end{bmatrix}_{n \\times (p+1)}\n",
    "\\begin{bmatrix}\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\\\ \\beta_p \\end{bmatrix}_{(p+1) \\times 1}\n",
    "=X\\beta\n",
    "$$\n",
    "\n",
    "As with convention, we'll use y to denote any 1-dimensional vector of output values or labels, and X to denote any matrix of input features.  Lets look at a simple linear regression model, with only one feature (p = 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create some fake data\n",
    "X = np.random.randn(100)\n",
    "y = (X+2)**2 + 5*np.random.random(len(X))\n",
    "plt.figure()\n",
    "plt.title('Sample regression problem with one input variable')\n",
    "plt.scatter(X,y, marker= 'o', s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training a scikit learn model follows these simple steps.\n",
    "\n",
    "#Import the classifier class we want\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#Initialize a classifier object\n",
    "#This will initiate it with defautl parameters \n",
    "cls = LinearRegression()\n",
    "\n",
    "#Fit the model based on the training data\n",
    "#First re-sahape X to be column rather than a flat array\n",
    "X = X.reshape(-1,1)\n",
    "cls.fit(X,y)\n",
    "\n",
    "#And that's it! \n",
    "#We can now use the fitted model to make predictions\n",
    "x_range = np.arange(-2.5,2.5,.1).reshape(-1,1)\n",
    "y_pred = cls.predict(x_range)\n",
    "\n",
    "#And we can plot this to see how well it matches\n",
    "plt.figure()\n",
    "plt.title('Sample regression problem with one input variable')\n",
    "plt.scatter(X,y, marker= 'o', s=50,label='Training Data')\n",
    "plt.plot(x_range,y_pred,'r',label='OLS Fit')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can also get the parameters of the fit\n",
    "print('linear model coeff (w): {}'.format(cls.coef_))\n",
    "print('linear model intercept (b): {:.3f}'.format(cls.intercept_))\n",
    "print('R-squared score (training): {:.3f}'.format(cls.score(X, y)))\n",
    "print('R-squared score (test): {:.3f}'.format(cls.score(X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use linear regression to fit higher order polynomials by mapping a single feature to higher order featuers.  That is, $y = \\beta_0 + \\beta_1 x$ becomes $y = \\beta_0 + \\beta_1 x + \\beta_2 x^2$ where $x$ and $x^2$ are now treated as two unique features by the ordinary least squares model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit higher order polynomial features\n",
    "from sklearn.preprocessing import PolynomialFeatures  \n",
    "poly = PolynomialFeatures(degree = 2)\n",
    "\n",
    "#Transform \n",
    "X_poly = poly.fit_transform(X) \n",
    "\n",
    "#Fit new model\n",
    "lin2 = LinearRegression()\n",
    "lin2.fit(X_poly,y)\n",
    "y_pred2 = lin2.predict(poly.fit_transform(x_range))\n",
    "\n",
    "#See if that matches data\n",
    "plt.figure()\n",
    "plt.title('Sample regression problem with one input variable')\n",
    "plt.scatter(X,y, marker= 'o', s=50,label='Training Data')\n",
    "plt.plot(x_range,y_pred2,'r',label='2nd Order Polynomial Fit')\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#And find the fit parameters\n",
    "print('linear model coeff (w): {}'.format(lin2.coef_))\n",
    "print('linear model intercept (b): {:.3f}'.format(lin2.intercept_))\n",
    "print('R-squared score (training): {:.3f}'.format(lin2.score(poly.fit_transform(X), y)))\n",
    "print('R-squared score (test): {:.3f}'.format(lin2.score(poly.fit_transform(X), y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quadratic fit performed much better, which makes sense since the data was generated from a quadratic model to begin with. \n",
    "\n",
    "For these models we just trained and any scikit-learn models, it also can be useful to save a version of the trained model so we can re-use it elsewhere without having to reload the data and re-train the model.  This can be accomplished with they `pickle` module as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save a copy of the trained polynomial model with pickle\n",
    "import pickle\n",
    "\n",
    "# Dump the trained model with Pickle\n",
    "pkl_filename = 'Data/trained_poly_model.pkl'\n",
    "# Open the file to save as pkl file\n",
    "model_pkl = open(pkl_filename, 'wb')\n",
    "pickle.dump(lin2, model_pkl)\n",
    "# Close the pickle instances\n",
    "model_pkl.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now wherever we have access to the pickeld file we can read back in the trained scikit model\n",
    "my_model_pkl = open(pkl_filename, 'rb')\n",
    "my_model = pickle.load(my_model_pkl)\n",
    "print (\"Loaded model :: \", my_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://dataaspirant.com/2017/02/13/save-scikit-learn-models-with-python-pickle/ for more of a primer on using pickle, which is useful for saving and storing many types of python objects, not just ML models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Reduction with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than use iris or mtcars data, lets look at an interesting sample data set of network traffic.  This data is from the Detection of [IoT Botnet Attacks dataset](https://archive.ics.uci.edu/ml/datasets/detection_of_IoT_botnet_attacks_N_BaIoT).  There is data for 9 IoT devices.  We will focus on one of these, SimpleHome XCS7 1003 WHT Security Camera, chosen because it has the most recent data (07-Jun-2018).\n",
    "\n",
    "For this SimpleHome Camera, we have about 18,000 examples of normal traffic.  We also have sample attack data for two types of botnets: Mirai and BASHLITE.  Each of these attack types consists of 5 specific classes of attacks:\n",
    "\n",
    "- <u>Mirai botnet attack types:</u> ack, scan, syn, udp, udpplain\n",
    "- <u>BASHLITE botnet attack types:</u> combo, junk, scan, tcp, udp\n",
    "\n",
    "There are 115 total features related to the flow of network traffic.  See link for full description of what these features represent.\n",
    "\n",
    "One other note on the data.  Each attack class is a 30-100MB csv file, containing from 20k-100k data points.  To keep the data more managable, we've already narrowed the attack data to a csv file containing 2000 data points from each attack class (e.g. 2000 for Mirai ack, 2000 for Mirai scan, etc).  Hence we have 10k total Mirai attacks, and 10k total Bashlite attacks, or 20k total attack records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in normal traffic\n",
    "df_normal = pd.read_csv('Data/normal_traffic.csv')\n",
    "print('Normal traffic has {} data points and {} features'.format(len(df_normal),\n",
    "                                                                df_normal.shape[1]))\n",
    "#Add label columns\n",
    "df_normal['label'] = 'normal'\n",
    "df_normal['attack_type'] = 'normal'\n",
    "df_normal.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in attack data\n",
    "df1 = pd.read_csv('Data/all_mirai_attacks.csv')\n",
    "df1['label'] = 'attack'\n",
    "df1['attack_type'] = 'mirai attack'\n",
    "print('{} mirai attacks data points'.format(len(df1)))\n",
    "df2 = pd.read_csv('Data/all_bashlite_attacks.csv')\n",
    "df2['label'] = 'attack'\n",
    "df2['attack_type'] = 'bashlite attack'\n",
    "print('{} bashlite attacks data points'.format(len(df2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine into a single dataframe\n",
    "df_all = pd.concat([df_normal,df1,df2])\n",
    "print(\"We have n = {} records and p = {} features.\".format(df_all.shape[0],df_all.shape[1]-2))\n",
    "print('\\nBreakdown by classes:')\n",
    "df_all['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets view the first two principle components with PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "#Reduce features to 2 PCA components\n",
    "#First, get just the feature values\n",
    "X_labeled = df_all.drop(['label','attack_type'],axis=1)\n",
    "#Scale all the features so they have mean of 0 and standard deviation of 1\n",
    "X_labeled_scaled = preprocessing.StandardScaler().fit_transform(X_labeled)\n",
    "#Convert this to a dataframe to facilitate plotting\n",
    "X_labeled_scaled = pd.DataFrame(X_labeled_scaled,columns=X_labeled.columns)\n",
    "#Now apply PCA\n",
    "n_components = 2\n",
    "pca = PCA(n_components=n_components)\n",
    "X_labeled_reduced = pca.fit_transform(X_labeled_scaled)\n",
    "X_labeled_reduced = pd.DataFrame(X_labeled_reduced)\n",
    "X_labeled_reduced = X_labeled_reduced.add_prefix('PC_')\n",
    "print('Using {} components accounts for {}% of variance.'.format(n_components,\n",
    "                                                                 100*sum(pca.explained_variance_ratio_)))\n",
    "print('Explained Variance Ratios of first two principle components:',pca.explained_variance_ratio_)\n",
    "\n",
    "#add labels back in\n",
    "df_labeled_reduced = X_labeled_reduced.copy()\n",
    "df_labeled_reduced['label'] = df_all['label'].values\n",
    "df_labeled_reduced['attack_type'] = df_all['attack_type'].values\n",
    "\n",
    "#reduce to managable sample\n",
    "df_labeled_sample = df_labeled_reduced.sample(frac=.02,random_state=0)\n",
    "\n",
    "#Now plot\n",
    "sns.FacetGrid(df_labeled_sample,hue=\"attack_type\").map(plt.scatter, \"PC_0\", \"PC_1\").add_legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Splits\n",
    "\n",
    "Scikit has built in tools to split data into training and test sets for testing and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split raw data into features (X) and target labels (y)\n",
    "X = df_all.drop(['label','attack_type'],axis=1)\n",
    "y = df_all['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here is the process to create a train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,random_state=0) \n",
    "#Don't need to include random_state, but useful to ensure we get the same results every time we run it\n",
    "print('{} samples in training set ({:.0%}), {} samples in test set ({:.0%})'.format(len(y_train),len(y_train)/len(y),\n",
    "                                                                                    len(y_test),len(y_test)/len(y)))\n",
    "\n",
    "#By default, train_test_split will shuffle the data to ensure train and test sets each has same portion of each class\n",
    "print('\\nFraction of each class in the test set:')\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, the default is to split 25% into the test set.  We can adjust this fraction , e.g. `train_test_split(X,y,test_size=0.1)` to get 10% into test set.\n",
    "\n",
    "For high dimension data like this, it also can be very helpful to reduce the data to a smaller number of features as a pre-processing step before running machine learning models.  As we'll see, this can greatly enhance the performance of many ML techniques, and also reduce the computational resources.  Lets also create a set of training and test data for reduced features.\n",
    "\n",
    "It turns out, when we want a training set of reduced data, we FIRST have to split the full data into training and test sets, and THEN reduce the data using PCA or some other feature reduction technique.  If we scaled and reduced the data and then did a train-test split, then we will have **data leakage** of test data into training data.  A big no-no. So lets apply PCA feature reduction to the training-test split we've already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As prep for PCA, scale the data so all features have mean 0 and variance of 1\n",
    "#NOTE: The scaling transform should be LEARNED from the training data, then applied as is to the test data.  \n",
    "#We don't re-fit the scaler on the test data.  \n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now pick number of principle components to reduce down to.  5 is a good number.\n",
    "n_components = 5\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(X_train_scaled)\n",
    "X_train_reduced = pca.transform(X_train_scaled)\n",
    "X_train_reduced = pd.DataFrame(X_train_reduced)\n",
    "X_train_reduced = X_train_reduced.add_prefix('PC_')\n",
    "\n",
    "X_test_reduced = pca.transform(X_test_scaled)\n",
    "X_test_reduced = pd.DataFrame(X_test_reduced)\n",
    "X_test_reduced = X_test_reduced.add_prefix('PC_')\n",
    "\n",
    "print('Note: Using {} PCA components accounts for {:.1%} of variance.'.format(n_components,\n",
    "                                                                 sum(pca.explained_variance_ratio_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a training and test data, lets run various classification algorithms and see how they do.  For the first example (Multi-Layer Perceptrons), we'll step through the modeling in detail including basic model training, evaluation criteria, and parameter tuning with grid search validation.  Then we'll quickly run through several other classification algorithms and see which one does best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Multi-Layer Perceptron Neural Nets\n",
    "\n",
    "Lets start with a type of neural network classifier, multi-layer perceptrons (go big or go home).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#The basics of training an MLP model\n",
    "\n",
    "#Load the package\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#Initialize a MLP Classifier object using default parameter values, call it cls\n",
    "cls = MLPClassifier()\n",
    "\n",
    "#Now fit the model on the training data\n",
    "cls.fit(X_train,y_train)\n",
    "\n",
    "#Now cls is a trained/fitted MLPClassifier object\n",
    "#We can use to to make predictions off of the test data (X_test) and see how well it matches \n",
    "y_predict = cls.predict(X_test)\n",
    "\n",
    "#Lets see how often the predictions match the actual labels\n",
    "acc = sum(pd.Series(y_predict).reset_index(drop=True) == pd.Series(y_test).reset_index(drop=True)) / len(y_test)\n",
    "print('Accuracy of trained MLP model on Test Set = {:.1%}'.format(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So far so good.  Scikit also has several built in tools to help evaluate the performance of classifiers.  In addition to accuracy, there are many other measure we can use, including:\n",
    "\n",
    "- Accuracy: What percentage of predictions match the test set\n",
    "- ROC AUC: Area under the ROC curve, useful way to think about a binary classifier\n",
    "- precision: Pr(Attack|\"Attack\"), where \"Attack\" means record is classified as an attack, and Attack means the record actually is an attack\n",
    "- recall: Pr(\"Attack\"|Attack), aka True Positive Rate (if attack is defined as positive)\n",
    "- F1 score: harmonic mean of precision and recall, $F1 = 2\\frac{precision \\bullet recall}{precision + recall}$\n",
    "- beta-f1 score: weighted harmonic mean of precision and recall.  This turns out to be a very useful metric in classification settings because the beta parameter roughly means that recall is $\\beta$ times as important as precision.  This can be applicable to network security applications where a low recall (meaning we fail to catch actual attacks, aka false negative) can be much worse than a low precision (meaning we mis-classify normal traffic as an attack, aka false positive).  See [F-Score description](https://en.wikipedia.org/wiki/F1_score).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Load the various classifier scoring functions\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Turns out for some of the scoring metrics, the outputs must be binary (0 or 1)\n",
    "y_test_bin = (y_test == 'attack').replace({True:1,False:0})\n",
    "y_predict_bin = (pd.Series(y_predict) == 'attack').replace({True:1,False:0})\n",
    "\n",
    "#Now compute the various evaluation metrics\n",
    "s1 = accuracy_score(y_test_bin,y_predict_bin)\n",
    "s2 = roc_auc_score(y_test_bin,y_predict_bin)\n",
    "s3 = precision_score(y_test,y_predict,pos_label = 'attack')\n",
    "s4 = recall_score(y_test,y_predict,pos_label = 'attack')\n",
    "s5 = f1_score(y_test,y_predict,pos_label = 'attack')\n",
    "s6 = fbeta_score(y_test,y_predict,beta=5,pos_label = 'attack')\n",
    "\n",
    "score_names = ['Accuracy','ROC AUC','precision','recall','F1','F5']\n",
    "df_scores = pd.DataFrame(columns=score_names)\n",
    "df_scores.loc[0] = [s1,s2,s3,s4,s5,s6]\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#We can also look at the confustion matrix, which shows following raw numbers:\n",
    "#  [[TP  FP]\n",
    "#    FN  TN]]\n",
    "cm = confusion_matrix(y_test,y_predict,labels=['normal','attack'])\n",
    "print(cm)\n",
    "print('\\nOut of {} attacks in test set, classifer failed to detect {}'.format(cm[1,1]+cm[1,0],cm[1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#The classification report is also useful\n",
    "print(classification_report(y_test,y_predict,digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So far we have been using the default parameters of the MLP classifier.  We can also tune the classifer parameters to try and achieve better performance.  The main parameter for MLP classifiers is the hidden layer sizes.  Lets define some possible values for this parameter then see which one does best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#This may take a couple minutes, get up and stretch\n",
    "\n",
    "hidden_layer_sizes_vals = [(10),(10,10),(20),(20,20)]\n",
    "#Length of tuple is number of layers; each value is number of neurons in that layer\n",
    "#So (10,10) means two layers with 10 neurons each\n",
    "\n",
    "#Initiate dataframe to hold all scores\n",
    "df_scores = pd.DataFrame(columns=['Hidden Layer Sizes','Accuracy','ROC AUC','precision','recall','F1','F5'])\n",
    "\n",
    "#Now see how they all do!\n",
    "for i,h in enumerate(hidden_layer_sizes_vals):\n",
    "    print('Training model # {} of {}'.format(i+1,len(hidden_layer_sizes_vals)))\n",
    "    nn = MLPClassifier(hidden_layer_sizes=h)\n",
    "    nn.fit(X_train,y_train)\n",
    "    y_predict = nn.predict(X_test)\n",
    "    \n",
    "    #find binary output vectors, which are needed for roc_auc_score\n",
    "    y_test_bin = (y_test == 'attack').astype(int) #convert True to 1, False to 0\n",
    "    y_predict_bin = (pd.Series(y_predict) == 'attack').astype(int)\n",
    "    \n",
    "    #find scores\n",
    "    s1 = accuracy_score(y_test_bin,y_predict_bin)\n",
    "    s2 = roc_auc_score(y_test_bin,y_predict_bin)\n",
    "    s3 = precision_score(y_test,y_predict,pos_label = 'attack')\n",
    "    s4 = recall_score(y_test,y_predict,pos_label = 'attack')\n",
    "    s5 = fbeta_score(y_test,y_predict,beta=1,pos_label = 'attack')\n",
    "    s6 = fbeta_score(y_test,y_predict,beta=5,pos_label = 'attack')\n",
    "\n",
    "    #Record Results\n",
    "    df_scores_new = pd.DataFrame(data={'Hidden Layer Sizes':[h],\n",
    "                                       'Accuracy':s1,\n",
    "                                       'ROC AUC':s2,\n",
    "                                       'precision':s3,\n",
    "                                       'recall':s4,\n",
    "                                       'F1':s5,\n",
    "                                       'F5':s6},index=[i])\n",
    "    df_scores = pd.concat([df_scores,df_scores_new],ignore_index=True)\n",
    "\n",
    "df_scores.sort_values('F5',ascending=False,inplace=True)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So far we trained on the full data set with all 115 features.  We can also try to train the models on the reduced data which contains only the 5 principle components.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Train on PCA-REDUCED data\n",
    "\n",
    "hidden_layer_sizes_vals = [(10),(10,10),(20),(20,20)]\n",
    "#Length of tuple is number of layers; each value is number of neurons in that layer\n",
    "#So (10,10) means two layers with 10 neurons each\n",
    "\n",
    "#Initiate dataframe to hold all scores\n",
    "df_scores = pd.DataFrame(columns=['Hidden Layer Sizes','Accuracy','ROC AUC','precision','recall','F1','F5'])\n",
    "\n",
    "#Now see how they all do!\n",
    "for i,h in enumerate(hidden_layer_sizes_vals):\n",
    "    print('Training model # {} of {}'.format(i+1,len(hidden_layer_sizes_vals)))\n",
    "    nn = MLPClassifier(hidden_layer_sizes=h)\n",
    "    nn.fit(X_train_reduced,y_train)\n",
    "    y_predict = nn.predict(X_test_reduced)\n",
    "    \n",
    "    #find binary output vectors, which are needed for roc_auc_score\n",
    "    y_test_bin = (y_test == 'attack').astype(int) #convert True to 1, False to 0\n",
    "    y_predict_bin = (pd.Series(y_predict) == 'attack').astype(int)\n",
    "    \n",
    "    #find scores\n",
    "    s1 = accuracy_score(y_test_bin,y_predict_bin)\n",
    "    s2 = roc_auc_score(y_test_bin,y_predict_bin)\n",
    "    s3 = precision_score(y_test,y_predict,pos_label = 'attack')\n",
    "    s4 = recall_score(y_test,y_predict,pos_label = 'attack')\n",
    "    s5 = fbeta_score(y_test,y_predict,beta=1,pos_label = 'attack')\n",
    "    s6 = fbeta_score(y_test,y_predict,beta=5,pos_label = 'attack')\n",
    "\n",
    "    #Record Results\n",
    "    df_scores_new = pd.DataFrame(data={'Hidden Layer Sizes':[h],\n",
    "                                       'Accuracy':s1,\n",
    "                                       'ROC AUC':s2,\n",
    "                                       'precision':s3,\n",
    "                                       'recall':s4,\n",
    "                                       'F1':s5,\n",
    "                                       'F5':s6},index=[i])\n",
    "    df_scores = pd.concat([df_scores,df_scores_new],ignore_index=True)\n",
    "\n",
    "df_scores.sort_values('F5',ascending=False,inplace=True)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Turns out that the model performed better when trained on reduced data.  It always strikes me as odd and counterintuitive that training on LESS INFORMATION (recall the first 5 components only accounted for 68% of the variance in the features) led to BETTER results.  By allowing the model to train on the most relevant information, turns out we can actually improved performance.  Moving forward we will train all subsequent models on the PCA-reduced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Before moving on, grab the best performance (the one that does best on F5 score, F score with beta = 5)\n",
    "best_scores = df_scores[score_names].iloc[0].tolist()\n",
    "best_scores_dict = {'MLP':best_scores}\n",
    "best_scores_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Logistic Regression\n",
    "\n",
    "Parameter Tuning:\n",
    "- C = regularization parameter.  Small C (.1) for more shrinkage/regularization; large C (100) for less.  More regularization would probably be useful when dealing with all 115 features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Parameter values to evalutate\n",
    "C_vals = [.01,.1,1,10,100]\n",
    "\n",
    "#Initiate dataframe to hold all scores\n",
    "df_scores = pd.DataFrame(columns=['C','Accuracy','ROC AUC','precision','recall','F1','F5'])\n",
    "\n",
    "#Now see how they all do!\n",
    "for i,c in enumerate(C_vals):\n",
    "    print('Training model # {} of {}'.format(i+1,len(C_vals)))\n",
    "    cls = LogisticRegression(C=c)\n",
    "    cls.fit(X_train_reduced,y_train)\n",
    "    y_predict = cls.predict(X_test_reduced)\n",
    "    \n",
    "    #find binary output vectors, which are needed for roc_auc_score\n",
    "    y_test_bin = (y_test == 'attack').astype(int) #convert True to 1, False to 0\n",
    "    y_predict_bin = (pd.Series(y_predict) == 'attack').astype(int)\n",
    "    \n",
    "    #find scores\n",
    "    s1 = accuracy_score(y_test_bin,y_predict_bin)\n",
    "    s2 = roc_auc_score(y_test_bin,y_predict_bin)\n",
    "    s3 = precision_score(y_test,y_predict,pos_label = 'attack')\n",
    "    s4 = recall_score(y_test,y_predict,pos_label = 'attack')\n",
    "    s5 = fbeta_score(y_test,y_predict,beta=1,pos_label = 'attack')\n",
    "    s6 = fbeta_score(y_test,y_predict,beta=5,pos_label = 'attack')\n",
    "\n",
    "    #Record Results\n",
    "    df_scores.loc[i] = [c,s1,s2,s3,s4,s5,s6]\n",
    "\n",
    "df_scores.sort_values('F5',ascending=False,inplace=True)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Record results\n",
    "best_scores = df_scores[score_names].iloc[0].tolist()\n",
    "best_scores_dict['Logistic Regression'] = best_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "Parameter tuning:\n",
    "- max_depth: how many splits to conduct per tree\n",
    "- criterion = ['gini','entropy], criteria to evaluate quality of a split at distinguishing between classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter values to evalutate\n",
    "criterion_vals = ['gini','entropy']\n",
    "max_depth_vals = [3,5]\n",
    "\n",
    "#Initiate dataframe to hold all scores\n",
    "df_scores = pd.DataFrame(columns=['criterion','max_depth','Accuracy','ROC AUC','precision','recall','F1','F5'])\n",
    "\n",
    "#Now see how they all do!\n",
    "i = 0\n",
    "for c in criterion_vals:\n",
    "    for d in max_depth_vals:\n",
    "        print('Training model # {}'.format(i+1))\n",
    "        cls = RandomForestClassifier(criterion=c,max_depth=d)\n",
    "        cls.fit(X_train_reduced,y_train)\n",
    "        y_predict = cls.predict(X_test_reduced)\n",
    "\n",
    "        #find binary output vectors, which are needed for roc_auc_score\n",
    "        y_test_bin = (y_test == 'attack').astype(int)\n",
    "        y_predict_bin = (pd.Series(y_predict) == 'attack').astype(int)\n",
    "\n",
    "        #find scores\n",
    "        s1 = accuracy_score(y_test_bin,y_predict_bin)\n",
    "        s2 = roc_auc_score(y_test_bin,y_predict_bin)\n",
    "        s3 = precision_score(y_test,y_predict,pos_label = 'attack')\n",
    "        s4 = recall_score(y_test,y_predict,pos_label = 'attack')\n",
    "        s5 = fbeta_score(y_test,y_predict,beta=1,pos_label = 'attack')\n",
    "        s6 = fbeta_score(y_test,y_predict,beta=5,pos_label = 'attack')\n",
    "\n",
    "        #Record Results\n",
    "        df_scores.loc[i] = [c,d,s1,s2,s3,s4,s5,s6]\n",
    "        i += 1\n",
    "\n",
    "df_scores.sort_values('F5',ascending=False,inplace=True)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Record results\n",
    "best_scores = df_scores[score_names].iloc[0].tolist()\n",
    "best_scores_dict['Random Forest'] = best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another good thing about random forests is that we get the feature importance\n",
    "features = X_train_reduced.columns\n",
    "feature_importances = cls.feature_importances_\n",
    "feature_ser = pd.Series(feature_importances,index=features)\n",
    "feature_ser.sort_values(ascending=False).iloc[:15]\n",
    "\n",
    "#Here it's just showing that the 1st principle compoment is most important, as expected\n",
    "#If we trained on real feature names this would be more useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameter values to evalutate\n",
    "criterion_vals = ['gini','entropy']\n",
    "max_depth_vals = [3,5]\n",
    "\n",
    "#Initiate dataframe to hold all scores\n",
    "df_scores = pd.DataFrame(columns=['criterion','max_depth','Accuracy','ROC AUC','precision','recall','F1','F5'])\n",
    "\n",
    "#Now see how they all do!\n",
    "i = 0\n",
    "for c in criterion_vals:\n",
    "    for d in max_depth_vals:\n",
    "        print('Training model # {}'.format(i+1))\n",
    "        cls = DecisionTreeClassifier(criterion=c,max_depth=d)\n",
    "        cls.fit(X_train_reduced,y_train)\n",
    "        y_predict = cls.predict(X_test_reduced)\n",
    "\n",
    "        #find binary output vectors, which are needed for roc_auc_score\n",
    "        y_test_bin = (y_test == 'attack').astype(int)\n",
    "        y_predict_bin = (pd.Series(y_predict) == 'attack').astype(int)\n",
    "\n",
    "        #find scores\n",
    "        s1 = accuracy_score(y_test_bin,y_predict_bin)\n",
    "        s2 = roc_auc_score(y_test_bin,y_predict_bin)\n",
    "        s3 = precision_score(y_test,y_predict,pos_label = 'attack')\n",
    "        s4 = recall_score(y_test,y_predict,pos_label = 'attack')\n",
    "        s5 = fbeta_score(y_test,y_predict,beta=1,pos_label = 'attack')\n",
    "        s6 = fbeta_score(y_test,y_predict,beta=5,pos_label = 'attack')\n",
    "\n",
    "        #Record Results\n",
    "        df_scores.loc[i] = [c,d,s1,s2,s3,s4,s5,s6]\n",
    "        i += 1\n",
    "\n",
    "df_scores.sort_values('F5',ascending=False,inplace=True)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Record results\n",
    "best_scores = df_scores[score_names].iloc[0].tolist()\n",
    "best_scores_dict['Decision Tree'] = best_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### K Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Parameter values to evalutate: number of neighbors used to assess most common class among neighbors\n",
    "k_vals = [3,5,10,15]\n",
    "\n",
    "#Initiate dataframe to hold all scores\n",
    "df_scores = pd.DataFrame(columns=['k','Accuracy','ROC AUC','precision','recall','F1','F5'])\n",
    "\n",
    "#Now see how they all do!\n",
    "i = 0\n",
    "for k in k_vals:\n",
    "    print('Training model # {}'.format(i+1))\n",
    "    cls = KNeighborsClassifier(n_neighbors=k)\n",
    "    cls.fit(X_train_reduced,y_train)\n",
    "    y_predict = cls.predict(X_test_reduced)\n",
    "\n",
    "    #find binary output vectors, which are needed for roc_auc_score\n",
    "    y_test_bin = (y_test == 'attack').astype(int)\n",
    "    y_predict_bin = (pd.Series(y_predict) == 'attack').astype(int)\n",
    "\n",
    "    #find scores\n",
    "    s1 = accuracy_score(y_test_bin,y_predict_bin)\n",
    "    s2 = roc_auc_score(y_test_bin,y_predict_bin)\n",
    "    s3 = precision_score(y_test,y_predict,pos_label = 'attack')\n",
    "    s4 = recall_score(y_test,y_predict,pos_label = 'attack')\n",
    "    s5 = fbeta_score(y_test,y_predict,beta=1,pos_label = 'attack')\n",
    "    s6 = fbeta_score(y_test,y_predict,beta=5,pos_label = 'attack')\n",
    "\n",
    "    #Record Results\n",
    "    df_scores.loc[i] = [k,s1,s2,s3,s4,s5,s6]\n",
    "    i += 1\n",
    "\n",
    "df_scores.sort_values('F5',ascending=False,inplace=True)\n",
    "df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#Record results\n",
    "best_scores = df_scores[score_names].iloc[0].tolist()\n",
    "best_scores_dict['KNN'] = best_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "##### Check out classifier results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "best_scores_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_all_scores = pd.DataFrame(data=best_scores_dict,index=score_names).transpose()\n",
    "df_all_scores.sort_values(by='Accuracy',ascending=False,inplace=True)\n",
    "df_all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Turns out KNN performed the best (in terms of F5 score, or F-beta score with beta = 5), though all classifiers did very well.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation with GridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we manually searched through different parameter values.  Scitkit has many tools that automate this task for us.  One very useful one is GridSearchCV which can tune the parameters of any classifier using cross validation.\n",
    "\n",
    "Here's how it would have worked for the Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since GridSearchCV performs its own cross validation, we can feed it the entire data set instead of just the training set, since each fold of the data acts as its own training-test split.  However one caution with GridSearch is that it doesn't automatically shuffle the data, so you'll want to shuffle it before feeding it in to make sure the training sets always contain a mix of all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mix the data up\n",
    "df_all = df_all.sample(frac=1)\n",
    "#Now grab feature data and target vector\n",
    "X = df_all.drop(['label','attack_type'],axis=1)\n",
    "y = df_all.label\n",
    "\n",
    "#Optionally scale and reduce the data back to 5 PCA dimensions\n",
    "# X_scaled = preprocessing.StandardScaler().fit_transform(X)\n",
    "# #Now apply pca\n",
    "# n_components = 5\n",
    "# pca = PCA(n_components=n_components)\n",
    "# X_reduced = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize classifier object\n",
    "cls = RandomForestClassifier(n_estimators=10)\n",
    "\n",
    "#Define parameters to cycle through\n",
    "criterion_vals = ['gini','entropy']\n",
    "max_depth_vals = [3,5]\n",
    "\n",
    "#Set up GridSearchCV object\n",
    "num_folds = 4 #number of cross validation folds\n",
    "grid_values = {'criterion':criterion_vals,'max_depth':max_depth_vals}\n",
    "grid_clf = GridSearchCV(cls,param_grid=grid_values,cv=num_folds)\n",
    "\n",
    "#Now fit it based on the full data set\n",
    "#GridSearch will automatically split the data set into folds\n",
    "grid_clf.fit(X,y)\n",
    "print('Best parameters:',grid_clf.best_params_)\n",
    "print('Best score:',grid_clf.best_score_)\n",
    "\n",
    "#Takes a minute or two to run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was an easy way to evaluate many parameters and find the best ones.  By default GridSearchCV uses accuracy to determine the best one, but we could also adjust that using scoring argument, which can take following options: ['f1-weighted','accuracy' (default),'roc_auc','recall','recall_micro'].  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that we can also retrieve the best classifier object as follows:\n",
    "best_cls = grid_clf.best_estimator_\n",
    "best_cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Learning / Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even when we don't have output values (y), and we only have inputs/features (X), simply clustering the data can be an insightful way to explore the data, identify patterns, or detect anomalies.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create some fake data\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X, y = make_blobs(random_state = 10,centers=4)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with K-Means clustering, a common clutering approach where points are initially assigned to some initial random clusters, then points are repeatedly assigned to whichever cluster a majority of their neighbors belong to until no points change cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import KMeans class\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#Load and fit a model\n",
    "kmeans = KMeans(n_clusters = 4)\n",
    "kmeans.fit(X)\n",
    "#Get the labels for each point\n",
    "labels = kmeans.labels_\n",
    "\n",
    "#And that's it!  See the reulting clusters\n",
    "df = pd.DataFrame({'x':X[:,0],'y':X[:,1],'Cluster':labels})\n",
    "sns.FacetGrid(df,hue=\"Cluster\").map(plt.scatter, \"x\", \"y\").add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we don't always know ahead of time how many clusters we are going to have.  A common approach is to try increasing the number of clusters in KMeans until you no longer see a significant decrease in the within cluster sum of squares (WCSS), a measure of the overall quality of the resulting cluster assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimum number of clusters for k-means classification\n",
    "n_cluster_vals = np.arange(1,11) # user specified, you can experiment...\n",
    "wcss = [] #Will hold within cluster sum of squares for each n_clusters\n",
    "\n",
    "for i in n_cluster_vals:\n",
    "    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, \n",
    "                    n_init = 10, random_state = 0)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "    \n",
    "#Plotting the results onto a line graph, allowing us to observe 'The elbow'\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(n_cluster_vals, wcss)\n",
    "plt.title('Using the elbow method to select number of clusters in KMeasn')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('WCSS') #within cluster sum of squares\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is called the \"elbow method\" because we see an elbow at the point where the WCSS measure of quality of fit no longer decreases with each additional cluster.\n",
    "\n",
    "Another common clustering method is Agglomerative Clustering, where every point starts as its own cluster, then points that are most similar are grouped together until some stopping criteria is me. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "cls = AgglomerativeClustering(n_clusters = 4)\n",
    "cls_assignment = cls.fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=65,c=cls_assignment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agglomerative clustering are also useful for viewing the dendorgram tree-like structures showing how individual objects/data points on the bottom are combined/agglomerated into clusters as we move up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "plt.figure()\n",
    "dendrogram(ward(X))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Density based clustering, where points either classified as part of a cluster or outliers depending on the density of points in their vicinity, is another popular clustering technique.  It is used widely in anomaly detection.  A popular method for density-based scanning is DBSCAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps = 2, min_samples = 2)\n",
    "cls_assignment = dbscan.fit_predict(X)\n",
    "plt.scatter(X[:, 0], X[:, 1], s=65,c=cls_assignment)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course this just scratches the surface of clustering techniques.  For a great overview of the theory behind these and other clustering techniques, see the two Clustering chapters in [Data Mining: Concepts and Techniques, 3rd Edition](https://learning.oreilly.com/library/view/data-mining-concepts/9780123814791/).  For a guide and examples of how to implement the algorithms, scitkit has many great [clustering examples](https://scikit-learn.org/stable/auto_examples/index.html#cluster-examples) on its website.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Data Analysis Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural Language Processing with nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[NLTK](https://www.nltk.org/) (natural language toolkit) has been the most common python library for natural language processing and text analysis, although other packages such as gensim and SpaCy are emerging as well.  To learn more, I'd recommend the Coursera course [Applied Text Mining in Python](https://www.coursera.org/learn/python-text-mining?specialization=data-science-python).  There are also many good online books on O'Reilly.  I'd recommend [Natural Language Process: Python and NLTK](https://learning.oreilly.com/library/view/natural-language-processing/9781787285101/), by Perkins, Mathur, Joshi, Hardeniya, Chopra as a good starting point to the main functionalities of NLTK.  [Applied Text Analysis with Python](https://learning.oreilly.com/library/view/applied-text-analysis/9781491963036/) by Bengfort, Bilbro, and Ojeda is also very good and goes beyond NLTK to cover the broader landscape of python text analytics tools (such as gensim for topic modeling and word embedding models), but requires more advanced python background from the reader.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Analysis with networkx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newtorkx has many powerful tools for the analysis, visualization, and any types of networks.  For a good primer I'd again recommend the coursera course [Applied Social Network Analysis in Python](https://www.coursera.org/learn/python-text-mining?specialization=data-science-python), or see the [documentation](https://networkx.github.io/documentation/stable/tutorial.html) for a good tutorial.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Modeling and Time Series Analysis with statsmodel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [statsmodel](https://www.statsmodels.org/stable/index.html) package has a wealth of statistical analysis tools, including regression models, Analysis of Variance (ANOVA), time series analysis (AR, ARIMA, etc), Kernel density estimation, and more.  \n",
    "\n",
    "Here is a good example using statsmodel for ARIMA time series analysis: https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [scipy](https://docs.scipy.org/doc/scipy/reference/optimize.html) libary has many built in methods for general optimization problems.  I also like the [cvxpy](https://www.cvxpy.org/) package since it has some added functionality.  For example, cvxpy makes it easier to conduct integer or binary programming where decision variables must be integers (or must be 0 or 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, most typical simulation in python is done with for loops.  However, there are some packages for more intense simulation work, such as [SimPy](https://realpython.com/simpy-simulating-with-python/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Association Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Association analysis, which aims to find correlations and linkages in large datasets, is arguably an under-utilized data mining tool since it tends to fall outside the bounds of machine learning, which is all anyone cares about these days.  Here's a good article on implementing association analysis with teh mlextend library: https://pbpython.com/market-basket-analysis.html.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
